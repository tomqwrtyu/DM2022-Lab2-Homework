{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import gensim\n",
    "import warnings\n",
    "import logging\n",
    "import ssl\n",
    "import urllib.request\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Raw Data loading\n",
    "tweets = pd.read_json('tweets_DM.json', lines=True)\n",
    "emotion = pd.read_csv('emotion.csv')\n",
    "trainOrTest = pd.read_csv('data_identification.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use dict to store 'idenification' and 'emotion' based on 'tweet_id'\n",
    "trainOrTestDict = {}; emoDict = {}\n",
    "for id, type in zip(trainOrTest['tweet_id'].values, trainOrTest['identification'].values): trainOrTestDict[id] = type\n",
    "for id, emo in zip(emotion['tweet_id'].values, emotion['emotion'].values): emoDict[id] = emo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ['score', 'hashtag', 'text', 'emotion'] #dataframe column label\n",
    "train_score_list = []\n",
    "train_hashtag_list = []\n",
    "train_text_list = []\n",
    "train_emotion_list = [] #train dataframe columns, including 'score', 'hashtag', 'text', 'emotion'\n",
    "\n",
    "test_id_list = []\n",
    "test_score_list = []\n",
    "test_hashtag_list = []\n",
    "test_text_list = [] #test dataframe columns, including 'id', 'score', 'text', 'hashtag'\n",
    "\n",
    "#however, I only used 'text', 'emotion' in train df and 'id', 'text' in test df\n",
    "\n",
    "for score, source in zip(tweets['_score'], tweets['_source']):\n",
    "    id = source['tweet']['tweet_id']\n",
    "    if trainOrTestDict[id] == 'train':\n",
    "        train_score_list.append(score)\n",
    "        train_hashtag_list.append(source['tweet']['hashtags'])\n",
    "        train_text_list.append(source['tweet']['text'].replace('<LH>','')) # Remove \"<LH>\" in text, because that was generated from Linux system,\n",
    "                                                                           # which should not occur in a normal sentence.\n",
    "        train_emotion_list.append(emoDict[id])\n",
    "    elif trainOrTestDict[id] == 'test':\n",
    "        test_id_list.append(id)\n",
    "        test_score_list.append(score)\n",
    "        test_hashtag_list.append(source['tweet']['hashtags'])\n",
    "        test_text_list.append(source['tweet']['text'].replace('<LH>','')) # Same operation as above\n",
    "        \n",
    "train_df = pd.DataFrame(np.array([train_score_list, train_hashtag_list, train_text_list, train_emotion_list]).T, columns=label)\n",
    "test_df = pd.DataFrame(np.array([test_id_list, test_score_list, test_hashtag_list, test_text_list]).T, columns=['id'] + label[:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing both text data\n",
    "train_df['tokenized'] = train_df['text'].apply(lambda x:nltk.word_tokenize(x))\n",
    "test_df['tokenized'] = test_df['text'].apply(lambda x:nltk.word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use LabelBinarizer to generate one-hot expression of emotion\n",
    "mlb = preprocessing.LabelBinarizer()\n",
    "mlb.fit(train_df['emotion'])\n",
    "train_df['one_hot_emotion'] = mlb.transform(train_df['emotion']).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save file, so next time won't do above again\n",
    "train_df.to_json('train.json')\n",
    "test_df.to_json('test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Try\n",
    "##### Sum the word embeddings of a sentence using pretrained glove-twitter-200 model, then train it with fully-connected NN\n",
    "##### Idea: Words with similar emotion should have similar word embeddings, so adding them up may outstand some features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data generated from above\n",
    "train_df = pd.read_json('train.json')\n",
    "test_df = pd.read_json('test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load pretrain model\n",
    "glove_twitter_200_model = api.load(\"glove-twitter-200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "NUM_CLASS = len(train_df['one_hot_emotion'][0])\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "#fully connected neural network\n",
    "def model(input_shape):\n",
    "    m = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(512, input_shape = input_shape, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.Dense(NUM_CLASS, activation='softmax'),\n",
    "    ])\n",
    "    m.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.categorical_crossentropy,\n",
    "              metrics=[tf.keras.metrics.CategoricalAccuracy(),\n",
    "                       tf.keras.metrics.Precision(),\n",
    "                       tf.keras.metrics.Recall()])\n",
    "    m.summary()\n",
    "    return m\n",
    "\n",
    "#Summation of word embeddings\n",
    "def handleInput(w2v_model, tokenized): \n",
    "    ret = []\n",
    "    sentenceVec = np.zeros(w2v_model['.'].shape)\n",
    "    for words in tokenized:\n",
    "        for word in words:\n",
    "            try:\n",
    "                sentenceVec = w2v_model[word.lower()]\n",
    "            except:\n",
    "                continue\n",
    "        ret.append(sentenceVec)\n",
    "    return np.array(ret)\n",
    "\n",
    "#function to generate output that will submit to Kaggle\n",
    "def generateOutput(X, out_path, m = None, model_path = 'Trial_fc'):\n",
    "    if not m:\n",
    "        m = tf.keras.models.load_model(model_path)\n",
    "    res = m.predict(X, batch_size=BATCH_SIZE).tolist()\n",
    "    mlb = preprocessing.LabelBinarizer()\n",
    "    mlb.fit(train_df['emotion'])\n",
    "    for i in range(len(res)):\n",
    "        top = max(res[i])\n",
    "        for j in range(len(res[i])):\n",
    "            if res[i][j] != top: res[i][j] = 0\n",
    "            else: res[i][j] = 1\n",
    "                \n",
    "    res = mlb.inverse_transform(np.array(res))\n",
    "    out_df = pd.DataFrame(np.concatenate([test_df['id'].to_numpy().astype(np.str_)[np.newaxis, :], res[np.newaxis, :]], axis = 0).T,\\\n",
    "                          columns=['id', 'emotion'])\n",
    "    out_df.to_csv(out_path, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#caculate the summation word embeddings based on their tokenized sentence\n",
    "X = handleInput(glove_twitter_200_model, train_df['tokenized'])\n",
    "tX = handleInput(glove_twitter_200_model, test_df['tokenized'])\n",
    "# save them, it takes some times and I don't want to wait for them every time.\n",
    "# np.save('X', X)\n",
    "# np.save('tX', tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loaddddddd\n",
    "X = np.load('X.npy')\n",
    "tX = np.load('tX.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_test_splittttt\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, np.array(train_df['one_hot_emotion'].to_list()), test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 512)               102912    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 8)                 2056      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 236,296\n",
      "Trainable params: 236,296\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#build model\n",
    "m = model(X_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train it!\n",
    "m.fit(x=X_train, y=y_train, epochs=20, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.evaluate(x=X_test, y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1610/1610 [==============================] - 3s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "generateOutput(tX, 'sub.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First try is somehow better (scoring 0.32886) than predicting all output as 'joy'(scoring 0.30765), and I think maybe fined-tune the word2vec model before extracting word embedding from it might get a more power solution.\n",
    "##### Note that the reason predicting all output as 'joy' is that 'joy' holding a great proportion in raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second try\n",
    "##### Train a roBERTa model mentioned in the class, and predicting the output by a fully-connected neural network.\n",
    "##### Idea: roBERTa sounds a powerful model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import regex as re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statistics\n",
    "import math\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tokenizers\n",
    "from transformers import RobertaTokenizer, TFRobertaModel\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import warnings\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of replicas: 1\n"
     ]
    }
   ],
   "source": [
    "# Detect hardware, return appropriate distribution strategy (you can see that it is pretty easy to set up).\n",
    "try:\n",
    "    # TPU detection. No parameters necessary if TPU_NAME environment variable is set (always set in Kaggle)\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print('Number of replicas:', strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = 'roberta-base'\n",
    "MAX_LEN = 64\n",
    "\n",
    "BATCH_SIZE = 64 * strategy.num_replicas_in_sync\n",
    "EPOCHS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>one_hot_emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>391</td>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>[People, who, post, ``, add, me, on, #, Snapch...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>433</td>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>[@, brianklaas, As, we, see, ,, Trump, is, dan...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>376</td>\n",
       "      <td>[]</td>\n",
       "      <td>Now ISSA is stalking Tasha 😂😂😂</td>\n",
       "      <td>fear</td>\n",
       "      <td>[Now, ISSA, is, stalking, Tasha, 😂😂😂]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>120</td>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>joy</td>\n",
       "      <td>[@, RISKshow, @, TheKevinAllison, Thx, for, th...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1021</td>\n",
       "      <td>[]</td>\n",
       "      <td>Still waiting on those supplies Liscus.</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>[Still, waiting, on, those, supplies, Liscus, .]</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score                        hashtag  \\\n",
       "0    391                     [Snapchat]   \n",
       "1    433  [freepress, TrumpLegacy, CNN]   \n",
       "2    376                             []   \n",
       "3    120      [authentic, LaughOutLoud]   \n",
       "4   1021                             []   \n",
       "\n",
       "                                                text       emotion  \\\n",
       "0  People who post \"add me on #Snapchat\" must be ...  anticipation   \n",
       "1  @brianklaas As we see, Trump is dangerous to #...       sadness   \n",
       "2                    Now ISSA is stalking Tasha 😂😂😂           fear   \n",
       "3  @RISKshow @TheKevinAllison Thx for the BEST TI...           joy   \n",
       "4           Still waiting on those supplies Liscus.   anticipation   \n",
       "\n",
       "                                           tokenized           one_hot_emotion  \n",
       "0  [People, who, post, ``, add, me, on, #, Snapch...  [0, 1, 0, 0, 0, 0, 0, 0]  \n",
       "1  [@, brianklaas, As, we, see, ,, Trump, is, dan...  [0, 0, 0, 0, 0, 1, 0, 0]  \n",
       "2              [Now, ISSA, is, stalking, Tasha, 😂😂😂]  [0, 0, 0, 1, 0, 0, 0, 0]  \n",
       "3  [@, RISKshow, @, TheKevinAllison, Thx, for, th...  [0, 0, 0, 0, 1, 0, 0, 0]  \n",
       "4   [Still, waiting, on, those, supplies, Liscus, .]  [0, 1, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_json('train.json')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_data = df[['text']].to_numpy().reshape(-1)\n",
    "y_data = df[['emotion']].to_numpy().reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize&Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def roberta_encode(texts, tokenizer):\n",
    "    ct = len(texts)\n",
    "    input_ids = np.ones((ct, MAX_LEN), dtype='int32')\n",
    "    attention_mask = np.zeros((ct, MAX_LEN), dtype='int32')\n",
    "    token_type_ids = np.zeros((ct, MAX_LEN), dtype='int32') # Not used in text classification\n",
    "\n",
    "    for k, text in enumerate(texts):\n",
    "        #tokenize\n",
    "        tok_text = tokenizer.tokenize(text)\n",
    "        \n",
    "        #truncate and convert tokens to numerical IDs\n",
    "        enc_text = tokenizer.convert_tokens_to_ids(tok_text[:(MAX_LEN-2)])\n",
    "        \n",
    "        input_length = len(enc_text) + 2\n",
    "        input_length = input_length if input_length < MAX_LEN else MAX_LEN\n",
    "        \n",
    "        #add tokens [CLS] and [SEP] at the beginning and the end\n",
    "        input_ids[k,:input_length] = np.asarray([0] + enc_text + [2], dtype='int32')\n",
    "        \n",
    "        #set to 1s in the attention input\n",
    "        attention_mask[k,:input_length] = 1\n",
    "\n",
    "    return {\n",
    "        'input_word_ids': input_ids,\n",
    "        'input_mask': attention_mask,\n",
    "        'input_type_ids': token_type_ids\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'anticipation',\n",
       " 1: 'sadness',\n",
       " 2: 'fear',\n",
       " 3: 'joy',\n",
       " 4: 'anger',\n",
       " 5: 'trust',\n",
       " 6: 'disgust',\n",
       " 7: 'surprise'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#transform categories into numbers\n",
    "category_to_id = {}\n",
    "category_to_name = {}\n",
    "\n",
    "for index, c in enumerate(y_data):\n",
    "    if c in category_to_id:\n",
    "        category_id = category_to_id[c]\n",
    "    else:\n",
    "        category_id = len(category_to_id)\n",
    "        category_to_id[c] = category_id\n",
    "        category_to_name[category_id] = c\n",
    "    \n",
    "    y_data[index] = category_id\n",
    "\n",
    "n_categories = len(list(category_to_name.keys()))\n",
    "category_to_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tokenizer from HuggingFace\n",
    "tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "#split into train and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.3, random_state=777) # random_state to reproduce results\n",
    "\n",
    "X_train = roberta_encode(X_train, tokenizer)\n",
    "X_test = roberta_encode(X_test, tokenizer)\n",
    "\n",
    "y_train = np.asarray(y_train, dtype='int32')\n",
    "y_test = np.asarray(y_test, dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save them \n",
    "np.save('roberta_xtrain', X_train)\n",
    "np.save('roberta_xtest', X_test)\n",
    "np.save('roberta_ytrain', y_train)\n",
    "np.save('roberta_ytest', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_categories = 8\n",
    "X_train = np.load('roberta_xtrain.npy', allow_pickle=True).item()\n",
    "X_test = np.load('roberta_xtest.npy', allow_pickle=True).item()\n",
    "y_train = np.load('roberta_ytrain.npy')\n",
    "y_test = np.load('roberta_ytest.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_model(n_categories):\n",
    "    with strategy.scope():\n",
    "        input_word_ids = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_word_ids')\n",
    "        input_mask = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_mask')\n",
    "        input_type_ids = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_type_ids')\n",
    "\n",
    "        # Import RoBERTa model from HuggingFace\n",
    "        roberta_model = TFRobertaModel.from_pretrained(MODEL_NAME)\n",
    "        x = roberta_model(input_word_ids, attention_mask=input_mask, token_type_ids=input_type_ids)\n",
    "\n",
    "        # Huggingface transformers have multiple outputs, embeddings are the first one,\n",
    "        # so let's slice out the first position\n",
    "        x = x[0]\n",
    "\n",
    "        x = tf.keras.layers.Dropout(0.1)(x)\n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "        x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "        x = tf.keras.layers.Dense(n_categories, activation='softmax')(x)\n",
    "\n",
    "        model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=x)\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(lr=1e-5),\n",
    "            loss=tf.metrics.sparse_categorical_crossentropy,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_word_ids (InputLayer)    [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " input_mask (InputLayer)        [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " input_type_ids (InputLayer)    [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_word_ids[0][0]',         \n",
      " el)                            thPoolingAndCrossAt               'input_mask[0][0]',             \n",
      "                                tentions(last_hidde               'input_type_ids[0][0]']         \n",
      "                                n_state=(None, 64,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)           (None, 64, 768)      0           ['tf_roberta_model[0][0]']       \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 49152)        0           ['dropout_37[0][0]']             \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 256)          12583168    ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 8)            2056        ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 137,230,856\n",
      "Trainable params: 137,230,856\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    model = build_model(n_categories)\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    print('Training...')\n",
    "    history = model.fit(X_train,\n",
    "                        y_train,\n",
    "                        epochs=EPOCHS,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        verbose=1,\n",
    "                        validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 68.20%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict and generate output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generatePrediciton(X_dut, id_dut, model, out_path):\n",
    "    y_pred = [category_to_name[np.argmax(i)] for i in model.predict(X_dut)]\n",
    "    df_out = pd.DataFrame(np.concatenate([id_dut[np.newaxis, :], np.array(y_pred)[np.newaxis, :]], axis=0).T, columns=['id', 'emotion'])\n",
    "    df_out.to_csv(out_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dut = pd.read_json('dut.json')\n",
    "id_dut = df_dut['id'].to_numpy().reshape(-1)\n",
    "#X_dut = roberta_encode(df_dut['text'].to_numpy().reshape(-1), tokenizer) #used for first time\n",
    "#np.save('roberta_xdut', X_dut)\n",
    "X_dut = np.load('roberta_xdut.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generatePrediciton(X_dut, id_dut, model, 'submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This method scored 0.56233 which is my best submission and the truth it was ranking third at that time satisfied me. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
